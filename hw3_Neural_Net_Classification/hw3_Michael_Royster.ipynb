{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data.tsv\", sep='\\t', on_bad_lines=\"skip\")\n",
    "df = df[['review_body', 'star_rating']]\n",
    "\n",
    "def categorize(x):\n",
    "    if x > 3:\n",
    "        return 3\n",
    "    elif x < 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# Form classes\n",
    "df = df[df['star_rating'].apply(lambda x: isinstance(x, int))]\n",
    "df = df[df['review_body'].str.split().str.len() > 2] # check for appropriate length, bumped to 2\n",
    "# df['review_split'] = df['review_body'].str.split()\n",
    "df['rating_class'] = df['star_rating'].apply(lambda x: categorize(x))\n",
    "df.drop(columns=['star_rating'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_one = df.query('rating_class == 1').sample(n=20_000).reset_index(drop=True)\n",
    "class_two = df.query('rating_class == 2').sample(n=20_000).reset_index(drop=True)\n",
    "class_three = df.query('rating_class == 3').sample(n=20_000).reset_index(drop=True)\n",
    "data_set = pd.concat([class_one, class_two, class_three]).reset_index(drop=True)\n",
    "\n",
    "data_set.to_csv(\"data_selection.csv\", index=False)\n",
    "data_set['review_split'] = data_set['review_body'].apply(lambda x: word_tokenize(x))\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_set = pd.read_csv(\"data_selection.csv\")\n",
    "# data_set['review_split'] = data_set['review_body'].apply(lambda x: word_tokenize(x))\n",
    "data_set['length'] = data_set['review_split'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def get_pos(x):\n",
    "    return x if x in ['n','v','a','r','s'] else 'n'\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data_set['review_body'] = data_set['review_body'].apply(lambda body: \" \".join([lemmatizer.lemmatize(w, get_pos(p[0].lower()))for w, p in pos_tag(nltk.word_tokenize(body), tagset='universal')]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman = Queen : 0.7300517559051514\n",
      "Excellent ~ Outstanding : 0.556748628616333\n",
      "Example 1:\n",
      "Foot - Leg = Hand - Arm 0.17670604586601257\n",
      "Example 2:\n",
      "Hill + Big = Mountain: 0.5208643674850464\n",
      "Example 3:\n",
      "Orange - Red + Blue = Green: 0.33706381916999817\n"
     ]
    }
   ],
   "source": [
    "def cosine_similarity(wv1, wv2):\n",
    "    return np.dot(wv1, wv2) / (np.linalg.norm(wv1) * np.linalg.norm(wv2))\n",
    "\n",
    "\"\"\" Come up with three examples\n",
    "    How to make sure words are in both vocabularies?\n",
    "\"\"\"\n",
    "ex1 = cosine_similarity(wv['king'] - wv['man'] + wv['woman'], wv['queen'])\n",
    "print(f\"King - Man + Woman = Queen : {ex1}\")\n",
    "ex2 = cosine_similarity(wv['excellent'], wv['outstanding'])\n",
    "print(f\"Excellent ~ Outstanding : {ex2}\")\n",
    "\n",
    "print(\"Example 1:\")\n",
    "print(f\"Foot - Leg = Hand - Arm {cosine_similarity(wv['foot'] - wv['leg'], wv['hand'] - wv['arm'])}\")\n",
    "print(\"Example 2:\")\n",
    "print(f\"Hill + Big = Mountain: {cosine_similarity(wv['hill'] + wv['big'], wv['mountain'])}\")\n",
    "print(\"Example 3:\")\n",
    "print(f\"Orange - Red + Blue = Green: {cosine_similarity(wv['orange'] - wv['red'] + wv['blue'], wv['green'])}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) Train word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "cm = gensim.models.Word2Vec(sentences=data_set['review_split'], vector_size=300, window=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King - Man + Woman = Queen : 0.08359397202730179\n",
      "Excellent ~ Outstanding: 0.6846609115600586\n",
      "Example 1:\n",
      "Foot - Leg = Hand - Arm 0.41826921701431274\n",
      "Example 2:\n",
      "Hill + Big = Mountain: 0.018984710797667503\n",
      "Example 3:\n",
      "Orange - Red + Blue = Green: 0.6284237504005432\n"
     ]
    }
   ],
   "source": [
    "\"\"\" TODO Compare to the previous three examples. \n",
    "    How to make sure words exist in both vocabularies?\n",
    "\"\"\"\n",
    "print(f\"King - Man + Woman = Queen : {cosine_similarity(cm.wv['king'] - cm.wv['man'] + cm.wv['woman'], cm.wv['queen'])}\")\n",
    "print(f\"Excellent ~ Outstanding: {cosine_similarity(cm.wv['excellent'], cm.wv['outstanding'])}\")\n",
    "\n",
    "print(\"Example 1:\")\n",
    "print(f\"Foot - Leg = Hand - Arm {cosine_similarity(cm.wv['foot'] - cm.wv['leg'], cm.wv['hand'] - cm.wv['arm'])}\")\n",
    "print(\"Example 2:\")\n",
    "print(f\"Hill + Big = Mountain: {cosine_similarity(cm.wv['hill'] + cm.wv['big'], cm.wv['mountain'])}\")\n",
    "print(\"Example 3:\")\n",
    "print(f\"Orange - Red + Blue = Green: {cosine_similarity(cm.wv['orange'] - cm.wv['red'] + cm.wv['blue'], cm.wv['green'])}\")\n",
    "\n",
    "\n",
    "# print(\"Example 1:\")\n",
    "# print(cosine_similarity(cm.wv['foot'] - cm.wv['leg'], cm.wv['hand'] - cm.wv['arm']))\n",
    "# print(\"Example 2:\")\n",
    "# print(cosine_similarity(cm.wv['hill'] + cm.wv['big'], cm.wv['mountain']))\n",
    "# print(\"Example 3:\")\n",
    "# print(cosine_similarity(cm.wv['orange'] - cm.wv['red'] + cm.wv['blue'], cm.wv['green']))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Simple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_w2v(wv, words):\n",
    "    avg = np.zeros(300)\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        try:\n",
    "            avg += wv[word]\n",
    "            count += 1\n",
    "        except:\n",
    "            pass\n",
    "    if not count: return avg\n",
    "    return avg / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_features(wv, words, num):\n",
    "    features = np.zeros((num, 300))\n",
    "    for i in range(num):\n",
    "        try:\n",
    "            features[i] = wv[words[i]]\n",
    "        except:\n",
    "            pass\n",
    "    return features.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_vectors = data_set['review_split'].apply(lambda x: avg_w2v(wv, x)).to_list()\n",
    "avg_values = pd.DataFrame(np.array(avg_vectors))\n",
    "w2v_model_avg = pd.concat([data_set, avg_values], axis=1)\n",
    "w2v_model_avg.drop(columns=['review_split', 'review_body', 'length'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(w2v_model_avg.query(\"rating_class == 1\"), test_size=0.2)\n",
    "train2, test2 = train_test_split(w2v_model_avg.query(\"rating_class == 2\"), test_size=0.2)\n",
    "train3, test3 = train_test_split(w2v_model_avg.query(\"rating_class == 3\"), test_size=0.2)\n",
    "\n",
    "avg_train = pd.concat([train1, train2, train3]).reset_index(drop=True)\n",
    "avg_test = pd.concat([test1, test2, test3]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.84      0.26      0.40      4000\n",
      "           2       0.46      0.81      0.59      4000\n",
      "           3       0.72      0.66      0.69      4000\n",
      "\n",
      "    accuracy                           0.58     12000\n",
      "   macro avg       0.67      0.58      0.56     12000\n",
      "weighted avg       0.67      0.58      0.56     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn import metrics\n",
    "\n",
    "w2v_perceptron = Perceptron()\n",
    "\n",
    "w2v_perceptron.fit(avg_train.iloc[0:avg_train.shape[0],list(range(1,avg_train.shape[1]))],\n",
    "                    avg_train['rating_class'])\n",
    "prediction = w2v_perceptron.predict(avg_test.iloc[0:avg_test.shape[0], list(range(1,avg_test.shape[1]))])\n",
    "w2v_results = pd.DataFrame(zip(avg_test.iloc[0:avg_test.shape[0], 0],prediction), columns=['Label', 'Prediction'])\n",
    "\n",
    "w2v_perceptron_report = metrics.classification_report(w2v_results['Label'], w2v_results['Prediction'], output_dict=True)\n",
    "print(metrics.classification_report(w2v_results['Label'], w2v_results['Prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.70      0.67      4000\n",
      "           2       0.58      0.53      0.55      4000\n",
      "           3       0.71      0.73      0.72      4000\n",
      "\n",
      "    accuracy                           0.65     12000\n",
      "   macro avg       0.65      0.65      0.65     12000\n",
      "weighted avg       0.65      0.65      0.65     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\" w2v SVM \"\"\"\n",
    "from sklearn import svm\n",
    "\n",
    "w2v_machine = svm.LinearSVC()\n",
    "\n",
    "w2v_machine.fit(avg_train.iloc[0:avg_train.shape[0],list(range(1,avg_train.shape[1]))],\n",
    "                    avg_train['rating_class'])\n",
    "prediction = w2v_machine.predict(avg_test.iloc[0:avg_test.shape[0], list(range(1,avg_test.shape[1]))])\n",
    "w2v_results = pd.DataFrame(zip(avg_test.iloc[0:avg_test.shape[0], 0],prediction), columns=['Label', 'Prediction'])\n",
    "\n",
    "w2v_svm_report = metrics.classification_report(w2v_results['Label'], w2v_results['Prediction'], output_dict=True)\n",
    "print(metrics.classification_report(w2v_results['Label'], w2v_results['Prediction']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train1, test1 = train_test_split(w2v_model_avg.query(\"rating_class == 1\"), test_size=0.2)\n",
    "# train2, test2 = train_test_split(w2v_model_avg.query(\"rating_class == 2\"), test_size=0.2)\n",
    "# train3, test3 = train_test_split(w2v_model_avg.query(\"rating_class == 3\"), test_size=0.2)\n",
    "\n",
    "# avg_train = pd.concat([train1, train2, train3]).reset_index(drop=True)\n",
    "# avg_test = pd.concat([test1, test2, test3]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = data_set['review_split'].apply(lambda x: first_features(wv, x, 10)).to_list()\n",
    "feature_values = pd.DataFrame(np.array(feature_vectors))\n",
    "w2v_model_first10 = pd.concat([data_set, feature_values], axis=1)\n",
    "w2v_model_first10.drop(columns=['review_split', 'review_body','length'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(w2v_model_first10.query(\"rating_class == 1\"), test_size=0.2)\n",
    "train2, test2 = train_test_split(w2v_model_first10.query(\"rating_class == 2\"), test_size=0.2)\n",
    "train3, test3 = train_test_split(w2v_model_first10.query(\"rating_class == 3\"), test_size=0.2)\n",
    "\n",
    "first10_train = pd.concat([train1, train2, train3]).reset_index(drop=True)\n",
    "first10_test = pd.concat([test1, test2, test3]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectors = data_set['review_split'].apply(lambda x: first_features(wv, x, 20)).to_list()\n",
    "feature_values = pd.DataFrame(np.array(feature_vectors))\n",
    "rnn_dataset = pd.concat([data_set, feature_values], axis=1)\n",
    "rnn_dataset.drop(columns=['review_split', 'review_body','length'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, test1 = train_test_split(rnn_dataset.query(\"rating_class == 1\"), test_size=0.2)\n",
    "train2, test2 = train_test_split(rnn_dataset.query(\"rating_class == 2\"), test_size=0.2)\n",
    "train3, test3 = train_test_split(rnn_dataset.query(\"rating_class == 3\"), test_size=0.2)\n",
    "\n",
    "rnn_train = pd.concat([train1, train2, train3]).reset_index(drop=True)\n",
    "rnn_test = pd.concat([test1, test2, test3]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_train.query(\"@rnn_train[3001] != 0.0\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train1, train2, train3, test1, test2, test3, data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, learning_rate, epochs, device, train_loader, test_loader, epsilon=0.0001):\n",
    "    model.cuda()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_accuracy = 0\n",
    "    for j in range(epochs):\n",
    "        model.train()\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        new_accuracy = predict(model, device, test_loader)['accuracy']\n",
    "        if (new_accuracy - epsilon < best_accuracy):\n",
    "            break\n",
    "        best_accuracy = max(best_accuracy, new_accuracy)\n",
    "        if j % 10 == 0:\n",
    "            print(f\"Epoch: {j}/{epochs}, Accuracy: {best_accuracy}\")\n",
    "    return model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, device, test_loader, verbose=False):\n",
    "    pred = []\n",
    "    actual = []\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "\n",
    "            pred = np.concatenate((pred, np.argmax(output.cpu().numpy(),axis=1)+1))\n",
    "            actual = np.concatenate((actual, np.argmax(y.cpu().numpy(),axis=1)+1))\n",
    "    \n",
    "    pred = [int(p) for p in pred]\n",
    "    actual = [int(a) for a in actual]\n",
    "    if verbose:\n",
    "        print(metrics.classification_report(actual, pred))\n",
    "    return metrics.classification_report(actual, pred, output_dict=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) Average word2vec vetors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(torch.nn.Module):\n",
    "    def __init__(self, D_in, h1, h2, D_out, dp1=0.5):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(D_in, h1, dtype=torch.float32)\n",
    "        self.d1 = torch.nn.Dropout(p=dp1)\n",
    "        self.a1 = torch.nn.ReLU()\n",
    "        self.l2 = torch.nn.Linear(h1, h2, dtype=torch.float32)\n",
    "        self.a2 = torch.nn.ReLU()\n",
    "        self.l3 = torch.nn.Linear(h2, D_out, dtype=torch.float32)\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        torch.nn.init.xavier_uniform_(self.l1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.l2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.l3.weight)\n",
    "        # torch.nn.init.zeros_(self.l1.bias)\n",
    "        # torch.nn.init.zeros_(self.l2.bias)\n",
    "        # torch.nn.init.zeros_(self.l3.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = self.d1(x)\n",
    "        x = self.a1(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.a2(x)\n",
    "        x = self.l3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2V_Dataset(data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        key = {1.0 : [1.0,0.0,0.0], 2.0: [0.0,1.0,0.0], 3.0: [0.0,0.0,1.0]}\n",
    "        self.x = torch.from_numpy(data[:, 1:])\n",
    "        self.y = torch.from_numpy(np.array([key[i] for i in data[:,0]]))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500, Accuracy: 0.6290833333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.68      0.67      4000\n",
      "           2       0.60      0.43      0.50      4000\n",
      "           3       0.64      0.80      0.71      4000\n",
      "\n",
      "    accuracy                           0.64     12000\n",
      "   macro avg       0.63      0.64      0.63     12000\n",
      "weighted avg       0.63      0.64      0.63     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = W2V_Dataset(avg_train.to_numpy(dtype=np.float32))\n",
    "x_test = W2V_Dataset(avg_test.to_numpy(dtype=np.float32))\n",
    "\n",
    "train_loader = data.DataLoader(dataset=x_train, batch_size=32, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=x_test, batch_size=4000, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# model = torch.load('mlp_model_avg.pth')\n",
    "model = MultiLayerPerceptron(300,100,10,3)\n",
    "model = train(model, learning_rate=0.001, epochs=500, device=device, train_loader=train_loader, test_loader=test_loader, epsilon=0.00001)\n",
    "ff_report = predict(model, device, test_loader, verbose=True)\n",
    "# 14 minutes for 500 epochs\n",
    "# torch.save(model, 'long_mlp_model_avg.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" Load mlp_model_avg \"\"\"\n",
    "# x_train = W2V_Dataset(avg_train.to_numpy(dtype=np.float32))\n",
    "# x_test = W2V_Dataset(avg_test.to_numpy(dtype=np.float32))\n",
    "\n",
    "# train_loader = data.DataLoader(dataset=x_train, batch_size=50, shuffle=True)\n",
    "# test_loader = data.DataLoader(dataset=x_test, batch_size=4000, shuffle=False)\n",
    "\n",
    "# model = torch.load('mlp_model_avg.pth')\n",
    "# report = predict(model, device, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) First 10 word2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500, Accuracy: 0.5513333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.58      0.52      0.55      4000\n",
      "           2       0.50      0.49      0.49      4000\n",
      "           3       0.61      0.69      0.65      4000\n",
      "\n",
      "    accuracy                           0.56     12000\n",
      "   macro avg       0.56      0.56      0.56     12000\n",
      "weighted avg       0.56      0.56      0.56     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train = W2V_Dataset(first10_train.to_numpy(dtype=np.float32))\n",
    "x_test = W2V_Dataset(first10_test.to_numpy(dtype=np.float32))\n",
    "\n",
    "train_loader = data.DataLoader(dataset=x_train, batch_size=32, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=x_test, batch_size=4000, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model = torch.load('mlp_first10.pth')\n",
    "model = MultiLayerPerceptron(3000,100,10,3)\n",
    "model = train(model, learning_rate=0.001, epochs=500, device=device, train_loader=train_loader, test_loader=test_loader, epsilon=0.0001)\n",
    "f10_report = predict(model, device, test_loader, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, 'long_mlp_first10.pth')\n",
    "# model = torch.load('mlp_first10.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" Load mlp_first10 \"\"\"\n",
    "# x_train = W2V_Dataset(first10_train.to_numpy(dtype=np.float32))\n",
    "# x_test = W2V_Dataset(first10_test.to_numpy(dtype=np.float32))\n",
    "\n",
    "# train_loader = data.DataLoader(dataset=x_train, batch_size=1000, shuffle=True)\n",
    "# test_loader = data.DataLoader(dataset=x_test, batch_size=4000, shuffle=False)\n",
    "\n",
    "# model = torch.load('mlp_first10.pth')\n",
    "# report = predict(model, device, test_loader)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
    "# num_layers?\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, D_in, hidden_size, output_size, dp=0.65):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = torch.nn.RNN(D_in, hidden_size, batch_first=True)\n",
    "        self.d = torch.nn.Dropout(p=dp)\n",
    "        self.l2 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h0 = self.init_hidden(x)\n",
    "        # output, new_hidden = self.rnn(x, h0)\n",
    "        output, new_hidden = self.rnn(x)\n",
    "        output = self.d(output) # dropout layer\n",
    "        # output = output[:, -1, :] # don't need first : if not batching\n",
    "        output = output[:,-1, :] \n",
    "        output = self.l2(output)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self,x):\n",
    "        return torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(model, learning_rate, epochs, device, train_loader, test_loader, batch_size, name, epsilon=0.00001):\n",
    "    model.cuda()\n",
    "    running_loss = 0\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    stagnate = 0\n",
    "    best_running_loss = 10_000\n",
    "    for j in range(epochs):\n",
    "        model.train()\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x = x.reshape(batch_size,20,300).to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "            loss = loss_fn(output, y)\n",
    "            \n",
    "            running_loss += loss.item() / x.size(0)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "        accuracy = predict_rnn(model, device, test_loader, 4000)['accuracy']\n",
    "        if (running_loss < best_running_loss):\n",
    "            best_running_loss = running_loss\n",
    "            stagnate = 0\n",
    "            torch.save(model, name)\n",
    "        else:\n",
    "            stagnate += 1\n",
    "\n",
    "        if (stagnate > 6):\n",
    "            break\n",
    "\n",
    "        if j % 10 == 0:\n",
    "            print(f\"Epoch: {j}/{epochs}, Accuracy: {accuracy} Running Loss: {running_loss}\")\n",
    "        running_loss = 0\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn(model, device, test_loader, batch_size, verbose=False):\n",
    "    pred = []\n",
    "    actual = []\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.reshape(batch_size, 20,300).to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            output = model(x)\n",
    "\n",
    "            pred = np.concatenate((pred, np.argmax(output.cpu().numpy(),axis=1)+1))\n",
    "            actual = np.concatenate((actual, np.argmax(y.cpu().numpy(),axis=1)+1))\n",
    "    \n",
    "    pred = [int(p) for p in pred]\n",
    "    actual = [int(a) for a in actual]\n",
    "    if verbose:\n",
    "        print(metrics.classification_report(actual, pred))\n",
    "    return metrics.classification_report(actual, pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500, Accuracy: 0.49425 Running Loss: 50.383006627249415\n",
      "Epoch: 10/500, Accuracy: 0.5541666666666667 Running Loss: 43.271946218092125\n",
      "Epoch: 20/500, Accuracy: 0.5751666666666667 Running Loss: 41.81841569132757\n",
      "Epoch: 30/500, Accuracy: 0.5964166666666667 Running Loss: 40.89222027138749\n",
      "Epoch: 40/500, Accuracy: 0.5883333333333334 Running Loss: 40.16052095900022\n",
      "Epoch: 50/500, Accuracy: 0.6045 Running Loss: 39.63026826425266\n",
      "Epoch: 60/500, Accuracy: 0.5978333333333333 Running Loss: 39.24327415881817\n",
      "Epoch: 70/500, Accuracy: 0.6045 Running Loss: 38.73418400674973\n",
      "Epoch: 80/500, Accuracy: 0.6031666666666666 Running Loss: 38.5470349576085\n",
      "Epoch: 90/500, Accuracy: 0.6070833333333333 Running Loss: 38.28171376479395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.60      0.63      0.62      4000\n",
      "           2       0.52      0.51      0.52      4000\n",
      "           3       0.69      0.67      0.68      4000\n",
      "\n",
      "    accuracy                           0.61     12000\n",
      "   macro avg       0.61      0.61      0.61     12000\n",
      "weighted avg       0.61      0.61      0.61     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "test_batch = 4000\n",
    "x_train = W2V_Dataset(rnn_train.to_numpy(dtype=np.float32))\n",
    "x_test = W2V_Dataset(rnn_test.to_numpy(dtype=np.float32))\n",
    "\n",
    "train_loader = data.DataLoader(dataset=x_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=x_test, batch_size=test_batch, shuffle=False)\n",
    "\n",
    "rnn_model = RNN(300,20,3)\n",
    "rnn_model = train_rnn(rnn_model, learning_rate=0.0005, epochs=500, device=device, train_loader=train_loader, test_loader=test_loader, batch_size=batch_size, name=\"best_rnn.pth\")\n",
    "rnn_report = predict_rnn(rnn_model, device, test_loader, batch_size=test_batch, verbose=True)\n",
    "\n",
    "# torch.save(rnn_model, 'long_simple_rnn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.62      0.61      4000\n",
      "           2       0.52      0.55      0.54      4000\n",
      "           3       0.70      0.65      0.68      4000\n",
      "\n",
      "    accuracy                           0.61     12000\n",
      "   macro avg       0.61      0.61      0.61     12000\n",
      "weighted avg       0.61      0.61      0.61     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn_model = torch.load(\"best_rnn.pth\")\n",
    "rnn_report = predict_rnn(rnn_model, device, test_loader, batch_size=test_batch, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) Simple RNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) Gated Recurrent Unit Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self, D_in, hidden_size, output_size, dp=0.65):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = torch.nn.GRU(D_in, hidden_size, batch_first=True)\n",
    "        self.d = torch.nn.Dropout(p=dp)\n",
    "        self.l2 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h0 = self.init_hidden(x)\n",
    "        # output, new_hidden = self.rnn(x, h0)\n",
    "        output, new_hidden = self.gru(x)\n",
    "        output = self.d(output)\n",
    "        # output = output[:, -1, :] # don't need first : if not batching\n",
    "        output = output[:,-1, :] \n",
    "        output = self.l2(output)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self,x):\n",
    "        return torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500, Accuracy: 0.6044166666666667 Running Loss: 43.18628474615332\n",
      "Epoch: 10/500, Accuracy: 0.6159166666666667 Running Loss: 36.86315435426533\n",
      "Epoch: 20/500, Accuracy: 0.6248333333333334 Running Loss: 36.19553748914076\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.62      0.62      0.62      4000\n",
      "           2       0.53      0.58      0.55      4000\n",
      "           3       0.72      0.65      0.68      4000\n",
      "\n",
      "    accuracy                           0.62     12000\n",
      "   macro avg       0.62      0.62      0.62     12000\n",
      "weighted avg       0.62      0.62      0.62     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "test_batch = 4000\n",
    "x_train = W2V_Dataset(rnn_train.to_numpy(dtype=np.float32))\n",
    "x_test = W2V_Dataset(rnn_test.to_numpy(dtype=np.float32))\n",
    "\n",
    "train_loader = data.DataLoader(dataset=x_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=x_test, batch_size=test_batch, shuffle=False)\n",
    "\n",
    "# gru_model = torch.load('long_gru.pth')\n",
    "gru_model = GRU(300,20,3)\n",
    "gru_model = train_rnn(gru_model, learning_rate=0.01, epochs=500, device=device, train_loader=train_loader, test_loader=test_loader, batch_size=batch_size, name=\"best_gru.pth\")\n",
    "gru_report = predict_rnn(gru_model, device, test_loader, batch_size=test_batch, verbose=True)\n",
    "\n",
    "# torch.save(gru_model, 'long_gru.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.65      0.53      0.59      4000\n",
      "           2       0.52      0.60      0.56      4000\n",
      "           3       0.69      0.71      0.70      4000\n",
      "\n",
      "    accuracy                           0.61     12000\n",
      "   macro avg       0.62      0.61      0.61     12000\n",
      "weighted avg       0.62      0.61      0.61     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gru_model = torch.load('best_gru.pth')\n",
    "gru_report = predict_rnn(gru_model, device, test_loader, batch_size=test_batch, verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (c) LSTM Unit Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, D_in, hidden_size, output_size, dp=0.65):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = torch.nn.LSTM(D_in, hidden_size, batch_first=True)\n",
    "        self.d = torch.nn.Dropout(p=dp)\n",
    "        self.l2 = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h0 = self.init_hidden(x)\n",
    "        # output, new_hidden = self.rnn(x, h0)\n",
    "        output, new_hidden = self.lstm(x)\n",
    "        output = self.d(output)\n",
    "        # output = output[:, -1, :] # don't need first : if not batching\n",
    "        output = output[:,-1, :] \n",
    "        output = self.l2(output)\n",
    "        return output\n",
    "\n",
    "    def init_hidden(self,x):\n",
    "        return torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/500, Accuracy: 0.5665833333333333 Running Loss: 47.0043888219173\n",
      "Epoch: 10/500, Accuracy: 0.6195 Running Loss: 36.580284813449\n",
      "Epoch: 20/500, Accuracy: 0.60425 Running Loss: 35.27969024093756\n",
      "Epoch: 30/500, Accuracy: 0.6093333333333333 Running Loss: 35.22206338035841\n",
      "Epoch: 40/500, Accuracy: 0.6101666666666666 Running Loss: 36.151444945751734\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.64      0.56      0.60      4000\n",
      "           2       0.51      0.58      0.54      4000\n",
      "           3       0.70      0.69      0.69      4000\n",
      "\n",
      "    accuracy                           0.61     12000\n",
      "   macro avg       0.62      0.61      0.61     12000\n",
      "weighted avg       0.62      0.61      0.61     12000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "test_batch = 4000\n",
    "x_train = W2V_Dataset(rnn_train.to_numpy(dtype=np.float32))\n",
    "x_test = W2V_Dataset(rnn_test.to_numpy(dtype=np.float32))\n",
    "\n",
    "train_loader = data.DataLoader(dataset=x_train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = data.DataLoader(dataset=x_test, batch_size=test_batch, shuffle=False)\n",
    "\n",
    "# lstm_model = torch.load('long_lstm.pth')\n",
    "lstm_model = LSTM(300,20,3)\n",
    "lstm_model = train_rnn(lstm_model, learning_rate=0.01, epochs=500, device=device, train_loader=train_loader, test_loader=test_loader, batch_size=batch_size, name=\"best_lstm.pth\")\n",
    "lstm_report = predict_rnn(lstm_model, device, test_loader, batch_size=test_batch, verbose=True)\n",
    "\n",
    "# torch.save(lstm_model, 'long_lstm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Accuracy ----------\n",
      "   Perceptron: 0.5771\n",
      "          SVM: 0.6527\n",
      "    FNN (avg): 0.6374\n",
      "FNN (first10): 0.5644\n",
      "   Simple RNN: 0.6066\n",
      "          GRU: 0.6143\n",
      "         LSTM: 0.6105\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"-\"*10, \"Accuracy\", \"-\"*10)\n",
    "print(f\"   Perceptron: {round(w2v_perceptron_report['accuracy'],4)}\")\n",
    "print(f\"          SVM: {round(w2v_svm_report['accuracy'],4)}\")\n",
    "print(f\"    FNN (avg): {round(ff_report['accuracy'],4)}\")\n",
    "print(f\"FNN (first10): {round(f10_report['accuracy'],4)}\")\n",
    "print(f\"   Simple RNN: {round(rnn_report['accuracy'],4)}\")\n",
    "print(f\"          GRU: {round(gru_report['accuracy'],4)}\")\n",
    "print(f\"         LSTM: {round(lstm_report['accuracy'],4)}\")\n",
    "print(f\"-\"*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6a7561f223c343ddefd33880ff283bbe6f27ee752ed949320b5281bca986dc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
